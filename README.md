# Intel-Unnati-Industrial Training Mistral AI 7B Chatbot

This is a fine-tuned GPT-based chatbot using the Mistral AI 7B Instruct model. The model has been fine-tuned on the Stanford Alpaca dataset and is available on Hugging Face. This project is a part of the Intel Unnati training - Introduction to GenAI and Simple LLM Inference on CPU, as well as the finetuning of the model.

## Table of Contents

1. [Overview](#overview)
2. [Installation Instructions](#installation-instructions)
3. [Usage](#usage)
4. [Model Information](#model-information)
5. [Dataset Information](#dataset-information)
6. [Fine-tuned Model Information](#fine-tuned-model-information)
7. [Acknowledgement](#acknowledgement)

## Overview
The Mistral AI 7B Instruct Chatbot is a fine-tuned version of the Mistral AI 7B Instruct model, which is a GPT-based language model. It has been fine-tuned on the Stanford Alpaca dataset and is available for use on Hugging Face. This project is a part of the Intel Unnati training - Intro to GenAI and Simple LLM Inference on CPU, as well as the finetuning of the model.

## Installation Instructions

To use this chatbot, you will need to follow these installation instructions:


1. Install the required dependencies:
```bash
pip install accelerate peft bitsandbytes git+https://github.com/huggingface/transformers trl py7zr auto-gptq optimum
```

2. Clone the repository:
```bash
git clone https://github.com/vanshmotiramani/mistral-finetuned-alpaca.git
```
3. Navigate to the project directory:
```bash
cd mistral-finetuned-alpaca
```
4. Install the Colab notebook dependencies:
```bash
Install the Colab notebook dependencies:
```
### Usage
To use the chatbot, you will need to follow these steps:

1. Install the Hugging Face CLI:
```bash 
pip install huggingface-cli
```
2. Authenticate with your Hugging Face account:
```bash
huggingface-cli login
```
3. Load the tokenizer and model:
```bash
from peft import AutoPeftModelForCausalLM
from transformers import GenerationConfig, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("vanshmotiramani/mistral-finetuned-alpaca")
model = AutoPeftModelForCausalLM.from_pretrained( "vanshmotiramani/mistral-finetuned-alpaca", low_cpu_mem_usage=True, return_dict=True, torch_dtype=torch.float16, device_map="cuda")
```
4. Define the generation configuration:
```bash
generation_config = GenerationConfig( do_sample=True, top_k=1, temperature=0.1, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id )
```
5. Generate a response from the chatbot:
```bash
inputs = tokenizer("###Human: What can GenAI do and how it could help world become a better place? ###Assistant: ", return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, generation_config=generation_config)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```
Replace the input text with your own questions or prompts.

### Model Information
The Mistral AI 7B Instruct model is a GPT-based language model with 7 billion parameters. It has been fine-tuned on the Stanford Alpaca dataset, making it a highly capable and versatile chatbot.

### Dataset Information
The Stanford Alpaca dataset is a collection of 52,002 instruction-following demonstrations generated by a SFT (Supervised Fine-Tuning) model. This dataset is used to fine-tune the Mistral AI 7B Instruct model.

### Fine-tuned Model Information
The fine-tuned model used in this chatbot is available on Hugging Face at **`vanshmotiramani/mistral-finetuned-alpaca`**. The fine-tuned model has been trained on the Stanford Alpaca dataset, making it a highly capable and versatile chatbot.
#### Dataset- [Finetuned-Alpaca-Dataset](https://huggingface.co/vanshmotiramani/mistral-finetuned-alpaca)

### Acknowledgement
This project is a part of the Intel Unnati training - Intro to GenAI and Simple LLM Inference on CPU, as well as the finetuning of the model. We would like to thank Intel Unnati for providing this opportunity and the resources to develop this chatbot. Additionally, we would like to acknowledge the original Mistral AI 7B Instruct model and the Stanford Alpaca dataset for their contributions to the field of language modeling.




